{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stereo SLAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a major part of this project and will likely take some time. \n",
    "\n",
    "For stereo, feel free to look up existing tutorials that implement this and write your own code here. Do not spend too long tweaking parameters here, focus on getting decent results and move on. You can also use OpenCV functions to backproject to 3D. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: Stereo dense reconstruction\n",
    "\n",
    "3-D point clouds are very useful in robotics for several tasks such as object detection, motion estimation (3D-3D matching or 3D-2D matching), SLAM, and other forms of scene understanding.  Stereo camerasprovide  us  with  a  convenient  way  to  generate  dense  point  clouds.Densehere,  in  contrast  tosparse,means all the image points are used for the reconstruction.  In this part of the assignment you will begenerating a dense 3D point cloud reconstruction of a scene from stereo images.\n",
    "\n",
    "#### Procedure: \n",
    "\n",
    "<ol>\n",
    "    <li> Generate a disparity map for each stereo pair.  Use OpenCV (e.g.  StereoSGBM) for this.  Notethat the images provided are already rectified and undistorted. </li>\n",
    "    <li> Then, using the camera parameters and baseline information generate colored point clouds from each disparity map.  Some points will have invalid disparity values, so ignore them.  Use [Open3D]for storing your point clouds. </li>\n",
    "    <li> Register (or transform) all the generated point clouds into your world frame by using the providedground truth poses. </li>\n",
    "    <li> Visualize the registered point cloud data, in color.  Use Open3D for this </li>\n",
    "</ol>\n",
    "    \n",
    "    \n",
    "Write briefly about how the disparity map is generated (if you used SGBM, write about SGBM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2021-12-02 20:34:49,106 - utils - NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import open3d as o3d\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/poses.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    T_list = []\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        tmp_rot = np.array(list(map(float, lines[i].split()))).reshape(-1, 4)\n",
    "        T_list.append(tmp_rot)\n",
    "    \n",
    "    T_list = np.array(T_list)\n",
    "    f.close()\n",
    "    \n",
    "with open('../data/calib.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    K = np.array(list(map(float, lines[1].split()))).reshape(3, 3)\n",
    "    b = float(lines[4].split()[0])\n",
    "    fl = K[0][0]\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_names = sorted(os.listdir('../data/img2'))\n",
    "\n",
    "pcd_g = o3d.geometry.PointCloud()\n",
    "pcd1 = o3d.geometry.PointCloud()\n",
    "pcd_list = []\n",
    "    \n",
    "for inum, img_n in enumerate(img_names):\n",
    "    imgl = cv2.imread(os.path.join('../data/img2', img_n))\n",
    "    imgr = cv2.imread(os.path.join('../data/img3', img_n))\n",
    "    \n",
    "    h, w, c = imgl.shape\n",
    "    win_size = 5\n",
    "    block_size = 5\n",
    "    min_disp = 10\n",
    "    num_disp = 100\n",
    "    \n",
    "    stereo = cv2.StereoSGBM_create(\n",
    "        minDisparity=min_disp,\n",
    "        numDisparities=num_disp,\n",
    "        blockSize=block_size,\n",
    "        uniquenessRatio=10,\n",
    "        speckleWindowSize=150,\n",
    "        speckleRange=32,\n",
    "        disp12MaxDiff=1,\n",
    "        P1=8*4*block_size**2,\n",
    "        P2=32*3*block_size**2,\n",
    "        preFilterCap=63\n",
    "    )\n",
    "    disp_map = stereo.compute(imgl, imgr).astype(np.float32) / 64\n",
    "#     disp_map = (disp_map - min_disp) / num_disp\n",
    "    disp_pts = []\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            disp_pts.append([j, i, disp_map[i,j], 1])\n",
    "    disp_pts = np.array(disp_pts)\n",
    "    \n",
    "    Q = np.float32([\n",
    "        [1, 0, 0, -613],\n",
    "        [0, -1, 0, 185],\n",
    "        [0, 0, 0, fl],\n",
    "        [0, 0, 1/b, 0]\n",
    "    ])\n",
    "    \n",
    "    points = (Q @ disp_pts.T).T\n",
    "    points /= points[:, 3:]\n",
    "    \n",
    "    colors = cv2.cvtColor(imgl, cv2.COLOR_BGR2RGB)\n",
    "    mask = disp_map >= disp_map.min()\n",
    "    colors = colors[mask]\n",
    "    colors = colors / 255\n",
    "    \n",
    "    world_pts = T_list[inum] @ points.T\n",
    "    world_pts[0] = -world_pts[0]\n",
    "    \n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(world_pts.T)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    \n",
    "    pcd_list.append(pcd)\n",
    "    pcd_g += pcd\n",
    "    \n",
    "    if inum == 0:\n",
    "        pcd1 += pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_new = pcd_g.voxel_down_sample(0.5)\n",
    "o3d.visualization.draw_geometries([pcd_new])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have projection matrix for transforming 2D points to 3D points and we have images from left camera and right camera, so we use stereo vision because stereo vision as we are comparing the scene from two vantage points. So as we have the image of a specific point from two different camera, we can find the actual location of that point, as we have pair of images and so as we know that the shift on the location of that point with respect to left camera on right camera is disparity. ( Disparity refers to the difference in the location of an object in corresponding two (left and right) images as seen by the left and right eye (or camera)which is created due to parallax (eyes‚Äô horizontal separation) )\n",
    "\n",
    "So firstly we calculate disparity of every stereo pair and store that in a matrix called disparity, this is done by a cv2 function steroSGBM_create() - this function uses an algorithm for matching blocks, and not individual pixels which increase the accuracy and the speed of our algo to calculate disparity\n",
    "\n",
    "Now we have to find the point cloud and for that firstly we need to find Q matrix, The Q matrix describes how the depth map is transformed into the 3D coordinate space. The constrction of the Q matrix is present in the function Q_arr given below\n",
    "\n",
    "We have with us, point_cloud_i =  (ùë•,ùë¶,ùëëùëñùë†ùëùùëéùëüùëñùë°ùë¶(ùë•,ùë¶),1)ùëá  - for the ith observation, this multiplied with Q gives us the point cloud.\n",
    "\n",
    "As we have to combine the mutliple point clouds, we multiply the every point_cloud of every stereo pair with the corresponding projection matrix(which is already given to us) to get all the final_points in a common frame, and we only take the point. After that we use mask to remove redundant points(noise), using the mask we are able to remove maximum number of redundant points(not all),so from that we will get our final point cloud which we will plot using open3d and use this point cloud in the next question to recover R and t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Motion estimation using iterative PnP\n",
    "\n",
    "Using the generated reconstruction from the previous part, synthesize a new image taken by a virtualmonocular camera fixed at any arbitrary position and orientation.  Your task in this part is to recoverthis pose using an iterative Perspective-from-n-Points (PnP) algorithm. \n",
    "\n",
    "#### Procedure: \n",
    "\n",
    "<ol>\n",
    "    <li> Obtain a set of 2D-3D correspondences between the the image and the point cloud.  Since hereyou‚Äôre generating the image, this should be easy to obtain. </li>\n",
    "    <li> For this set of correspondences compute the total reprojection error c= $\\sum_{i} ‚Äñx_i‚àíP_{k}X_i‚Äñ^2 $    where $P_{k}= K[R_{k}|t_{k}]$, $X_{i}$ is the 3D point in the world frame, $x_{i}$ is its corresponding projection. </li>\n",
    "    <li> Solve for the pose $T_{k}$ that minimizes this non-linear reprojection error using a Gauss-Newton (GN)scheme.  Recall that in GN we start with some initial estimated value $x_{o}$ and iteratively refine the estimate using $x_{1}$= $‚àÜx+x_0$, where $‚àÜx$ is obtained by solving the normal equations $J^{T}J‚àÜx$= -$J^{T}e$, until convergence.The main steps in this scheme are computing the corresponding Jacobians and updating the estimates correctly.  For our problem,  use a 12√ó1 vector parameterization for $T_{k}$(the top 3√ó4submatrix).  Run the optimization for different choices of initialization and report your observations. </li>\n",
    "</ol>\n",
    "\n",
    "Make sure that you write about how you calculate the residual and jacobians. Do not just include the code. The pose that you have been given is the ground truth, so using that will obviously give good results for optimization, so try out something else as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.20174382e+01 -7.24927376e+02  8.78437161e+01  7.20706353e+02]\n",
      " [-3.69357861e+01  7.13517079e+02 -1.52329188e+02 -9.41615093e-02]\n",
      " [-2.45618066e-01 -1.00289213e-01 -8.48121684e-01  1.30824170e-01]]\n"
     ]
    }
   ],
   "source": [
    "pts = np.asarray(pcd1.points)\n",
    "pts_hom = np.hstack((pts, np.ones((pts.shape[0], 1))))\n",
    "\n",
    "t = np.linalg.inv(np.vstack((T_list[0], np.ones((1, 4)))))\n",
    "P_gt = K @ t[:3, :]\n",
    "\n",
    "px = (P_gt @ pts_hom.T).T\n",
    "px /= px[:, 2:]\n",
    "px = px[:, :2]\n",
    "\n",
    "print(P_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DLT(image, world):\n",
    "    G = []\n",
    "    n, _ = image.shape\n",
    "    \n",
    "    for i in range(n):\n",
    "        X_i = world[i][0]\n",
    "        Y_i = world[i][1]\n",
    "        Z_i = world[i][2]\n",
    "    \n",
    "        u_i = image[i][0]\n",
    "        v_i = image[i][1]\n",
    "        \n",
    "        G.append([X_i, Y_i, Z_i, 1, 0, 0, 0, 0, -u_i*X_i, -u_i*Y_i, -u_i*Z_i, -u_i])\n",
    "        G.append([0, 0, 0, 0, X_i, Y_i, Z_i, 1, -v_i*X_i, -v_i*Y_i, -v_i*Z_i, -v_i])\n",
    "\n",
    "    U, S, Vh = np.linalg.svd(G)\n",
    "    \n",
    "    P = Vh[-1, :] / (Vh[-1, -1] + 1e-8)\n",
    "    P = P.reshape(3, 4)\n",
    "    \n",
    "    return P\n",
    "\n",
    "def get_Jr(x, P):\n",
    "    y = (P @ (np.hstack((x, np.ones((x.shape[0], 1))))).T).T\n",
    "    \n",
    "    J = []\n",
    "    for i in range(x.shape[0]):\n",
    "        J_x = [x[i, 0]/y[i, 2], x[i, 1]/y[i, 2], x[i, 2]/y[i, 2], 1/y[i, 2], 0, 0, 0, 0, -x[i, 0]*y[i, 0]/(y[i, 2]**2), -x[i, 1]*y[i, 0]/(y[i, 2]**2), -x[i, 2]*y[i, 0]/(y[i, 2]**2), -y[i, 0]/(y[i, 2]**2)]\n",
    "        J_y = [0, 0, 0, 0, x[i, 0]/y[i, 2], x[i, 1]/y[i, 2], x[i, 2]/y[i, 2], 1/y[i, 2], -x[i, 0]*y[i, 1]/(y[i, 2]**2), -x[i, 1]*y[i, 1]/(y[i, 2]**2), -x[i, 2]*y[i, 1]/(y[i, 2]**2), -y[i, 1]/(y[i, 2]**2)]\n",
    "        \n",
    "        J.append(J_x)\n",
    "        J.append(J_y)\n",
    "        \n",
    "    return np.array(J)\n",
    "\n",
    "def get_res(x, P, y):\n",
    "    y_tmp = (P @ np.hstack((x, np.ones((x.shape[0], 1)))).T).T\n",
    "    y_tmp /= y_tmp[:, 2:]\n",
    "    y_tmp = y_tmp[:, :2]\n",
    "    \n",
    "    return (y - y_tmp).reshape(-1, 1)\n",
    "\n",
    "\n",
    "def training_GN(x, y, P, args):\n",
    "    costs = []\n",
    "    for ep in range(args[\"num_epochs\"]):\n",
    "        res = get_res(x, P, y)\n",
    "        costs.append(np.linalg.norm(res)**2)\n",
    "        if (ep != 0) and (abs(costs[-1] - costs[-2]) < args[\"tol\"]):\n",
    "            print(\"Tolerance reached at epoch:\", ep)\n",
    "            break\n",
    "            \n",
    "        J_r = get_Jr(x, P)\n",
    "        J_F = np.linalg.inv(J_r.T @ J_r) @ J_r.T @ res\n",
    "        del_p = -args[\"alpha\"] * J_F\n",
    "        \n",
    "        P += del_p.reshape(3, 4)\n",
    "        \n",
    "    return P, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.67742856e+02 -5.54200098e+03  6.69554293e+02  5.50966825e+03]\n",
      " [-2.82674323e+02  5.45441204e+03 -1.16558467e+03 -6.77923763e-01]\n",
      " [-1.87764783e+00 -7.66669267e-01 -6.48353708e+00  1.00009630e+00]]\n"
     ]
    }
   ],
   "source": [
    "p_init = DLT(px[:5000], pts[:5000])\n",
    "args = {\n",
    "    \"num_epochs\": 10,\n",
    "    \"tol\": 1e-3,\n",
    "    \"alpha\": 0.1\n",
    "}\n",
    "\n",
    "P_GN, costs = training_GN(pts, px, p_init, args)\n",
    "print(P_GN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.67822928e+02 -5.54196834e+03  6.69820574e+02  5.50880075e+03]\n",
      " [-2.82207302e+02  5.45460236e+03 -1.16398176e+03 -1.70046003e+00]\n",
      " [-1.87764783e+00 -7.66669266e-01 -6.48353707e+00  1.00009630e+00]]\n"
     ]
    }
   ],
   "source": [
    "p_init = DLT(px[:2500], pts[:2500])\n",
    "args = {\n",
    "    \"num_epochs\": 10,\n",
    "    \"tol\": 1e-3,\n",
    "    \"alpha\": 0.1\n",
    "}\n",
    "\n",
    "P_GN, costs = training_GN(pts, px, p_init, args)\n",
    "print(P_GN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tolerance reached at epoch: 1\n",
      "[[ 2.20174382e+01 -7.24927376e+02  8.78437161e+01  7.20706353e+02]\n",
      " [-3.69357861e+01  7.13517079e+02 -1.52329188e+02 -9.41615093e-02]\n",
      " [-2.45618066e-01 -1.00289213e-01 -8.48121684e-01  1.30824170e-01]]\n"
     ]
    }
   ],
   "source": [
    "p_init = P_gt\n",
    "args = {\n",
    "    \"num_epochs\": 10,\n",
    "    \"tol\": 1e-3,\n",
    "    \"alpha\": 0.1\n",
    "}\n",
    "\n",
    "P_GN, costs = training_GN(pts, px, p_init, args)\n",
    "print(P_GN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.18078172e+04 -3.42122006e+03  8.46149428e+04  2.37718921e+05]\n",
      " [-8.01132453e+03  1.15069575e+04 -2.79657418e+04  1.70335258e+05]\n",
      " [-1.75671034e+00 -8.37154256e-01 -6.55608797e+00  1.04300567e+00]]\n"
     ]
    }
   ],
   "source": [
    "p_init = DLT(px[:5000], pts[:5000]) + np.random.normal(0, 0.1, (3, 4))\n",
    "args = {\n",
    "    \"num_epochs\": 10,\n",
    "    \"tol\": 1e-3,\n",
    "    \"alpha\": 0.1\n",
    "}\n",
    "\n",
    "P_GN, costs = training_GN(pts, px, p_init, args)\n",
    "print(P_GN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.54390879e+05 -3.54061768e+04 -4.29646137e+05  3.39618582e+04]\n",
      " [ 9.17120092e+03  3.75598738e+03  2.60664595e+04  4.19783785e+04]\n",
      " [-2.85527687e-01 -6.27261104e-02 -7.94609516e-01  7.32076192e-02]]\n"
     ]
    }
   ],
   "source": [
    "p_init = P_gt + np.random.normal(0, 0.1, (3, 4))\n",
    "args = {\n",
    "    \"num_epochs\": 10,\n",
    "    \"tol\": 1e-3,\n",
    "    \"alpha\": 0.1\n",
    "}\n",
    "\n",
    "P_GN, costs = training_GN(pts, px, p_init, args)\n",
    "print(P_GN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above set of experimentation, we can see that DLT initialisation gives best possible final optimised P matrix and transformation(T) matrix. Adding noise(however small) hugely distorts output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3: Odometry Calculation\n",
    "\n",
    "In part 1, you used the ground truth pose for registration. Here, try to estimate the pose using the RGB image data alone. \n",
    "\n",
    "#### Procedure:\n",
    "\n",
    "1. This can be done by computing features across the two images and matching them. Since you already have the depth map, you now have correspondences between the depth maps of two images as well from the RGB feature matches. \n",
    "2. You can now convert this depth map to a point cloud.\n",
    "3. Since you have correspondences between image points in the depth map, you have 3D correspondences here as well. Perform ICP here to get a good pose estimate.\n",
    "4. Feed these initial pose estimates into the PnP pipeline and optimise further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "import copy\n",
    "\n",
    "# Function to obtain procrustes alignment\n",
    "def orthogonal_procrustes_alignment(pts_1, pts_2):\n",
    "    # n -> number of points in pcd\n",
    "    # d -> number of dimensions (3)\n",
    "    n, d = pts_1.shape\n",
    "    \n",
    "    # Centroids of both point clouds\n",
    "    centroid_1 = np.mean(pts_1, axis=0)\n",
    "    centroid_2 = np.mean(pts_2, axis=0)\n",
    "    \n",
    "    # Normalized points (points - center)\n",
    "    pts_1_normalized = pts_1 - centroid_1\n",
    "    pts_2_normalized = pts_2 - centroid_2\n",
    "    \n",
    "    # SVD\n",
    "    H = pts_1_normalized.T @ pts_2_normalized\n",
    "    U, S, Vt = np.linalg.svd(H)\n",
    "    \n",
    "    # Rotation matrix\n",
    "    R = U @ Vt\n",
    "    \n",
    "    if np.linalg.det(R) < 0:\n",
    "        Vt[-1, :] *= -1\n",
    "        R = U @ Vt\n",
    "    \n",
    "    # Translation vector\n",
    "    t = centroid_1 - R @ centroid_2\n",
    "    \n",
    "    # Transformation matrix\n",
    "    T = np.eye(d+1)\n",
    "    T[:d, :d] = R\n",
    "    T[:d, d] = t\n",
    "    \n",
    "    return T, R, t\n",
    "\n",
    "def ICP_alignment(pts_1, pts_2, num_iters=10, tol=0.0001):\n",
    "    pts1 = copy.deepcopy(pts_1)\n",
    "    pts2 = copy.deepcopy(pts_2)\n",
    "    \n",
    "    # Previous iteration distance\n",
    "    dist_prev = np.mean(np.abs(pts1 - pts2))\n",
    "    \n",
    "    # Transformation matrix\n",
    "    T_accum = np.eye(pts1.shape[1]+1)\n",
    "    \n",
    "    for iter in range(num_iters):\n",
    "        # Find correspondences\n",
    "        tree = KDTree(pts1)\n",
    "        distances, indices = tree.query(pts2, 1)\n",
    "        distances = distances.squeeze()\n",
    "        indices = indices.squeeze()\n",
    "        \n",
    "        # Check for tolerance\n",
    "        if (iter != 0) and (np.abs(np.mean(distances) - dist_prev) < tol):\n",
    "            print(\"Tolerance reached!!\")\n",
    "            break\n",
    "        else:\n",
    "            dist_prev = np.mean(distances)\n",
    "        \n",
    "        # Obtain pts2 according to correspondences\n",
    "        pts2_nearest = pts2[indices, :]\n",
    "        # Obtain procrusted alignment\n",
    "        T, R, t = orthogonal_procrustes_alignment(pts1, pts2_nearest)\n",
    "        \n",
    "        # Accumulate transformation matrix\n",
    "        T_accum = T_accum @ np.linalg.inv(T)\n",
    "        \n",
    "        # Apply T to pts2 to update\n",
    "        pts2 = (R @ pts2.T).T + t\n",
    "        \n",
    "        print(\"Iteration: \" + str(iter) + \", Distance: \" + str(dist_prev))\n",
    "            \n",
    "    return T_accum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgl = cv2.imread(os.path.join('../data/img2', img_names[0]))\n",
    "imgr = cv2.imread(os.path.join('../data/img3', img_names[0]))\n",
    "\n",
    "orb = cv2.ORB_create()\n",
    "kp1, des1 = orb.detectAndCompute(imgl, None)\n",
    "kp2, des2 = orb.detectAndCompute(imgr, None)\n",
    "\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(des1, des2)\n",
    "matches = sorted(matches, key = lambda x: x.distance)\n",
    "\n",
    "inds1 = [x.queryIdx for x in matches]\n",
    "inds2 = [x.trainIdx for x in matches]\n",
    "\n",
    "pts1 = []\n",
    "for ind in inds1:\n",
    "    x, y = map(int, kp1[ind].pt)\n",
    "    pts1.append(imgl[y, x])\n",
    "\n",
    "pts1 = np.array(pts1)\n",
    "\n",
    "pts2 = []\n",
    "for ind in inds2:\n",
    "    x, y = map(int, kp2[ind].pt)\n",
    "    pts2.append(imgr[y, x])\n",
    "\n",
    "pts2 = np.array(pts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Distance: 11.544798176559897\n",
      "Iteration: 1, Distance: 18.295068369998383\n",
      "Iteration: 2, Distance: 20.62538591344898\n",
      "Iteration: 3, Distance: 18.88588458567366\n",
      "Iteration: 4, Distance: 26.929604417181025\n",
      "Iteration: 5, Distance: 19.199455584651375\n",
      "Iteration: 6, Distance: 16.15938819357187\n",
      "Iteration: 7, Distance: 18.22635691775938\n",
      "Iteration: 8, Distance: 18.67223151362586\n",
      "Iteration: 9, Distance: 15.944766388977687\n",
      "Iteration: 10, Distance: 13.873184221830556\n",
      "Iteration: 11, Distance: 18.7913791137495\n",
      "Iteration: 12, Distance: 13.878277048776532\n",
      "Iteration: 13, Distance: 42.333913574009394\n",
      "Iteration: 14, Distance: 22.8443039697182\n",
      "Iteration: 15, Distance: 17.286855111805163\n",
      "Iteration: 16, Distance: 19.183484886152353\n",
      "Iteration: 17, Distance: 18.684606697298424\n",
      "Iteration: 18, Distance: 28.983981996431318\n",
      "Iteration: 19, Distance: 17.087796082877656\n",
      "Iteration: 20, Distance: 15.406454886638667\n",
      "Iteration: 21, Distance: 15.363774841608496\n",
      "Iteration: 22, Distance: 21.872778223969213\n",
      "Iteration: 23, Distance: 17.50431955374692\n",
      "Iteration: 24, Distance: 16.835429219957373\n",
      "Iteration: 25, Distance: 13.312896864075626\n",
      "Iteration: 26, Distance: 14.178478140873043\n",
      "Iteration: 27, Distance: 19.659392290111867\n",
      "Iteration: 28, Distance: 15.081029225637137\n",
      "Iteration: 29, Distance: 19.08147834047062\n",
      "Iteration: 30, Distance: 17.35160362638629\n",
      "Iteration: 31, Distance: 19.919278840908703\n",
      "Iteration: 32, Distance: 24.061973362861885\n",
      "Iteration: 33, Distance: 22.50282290233914\n",
      "Iteration: 34, Distance: 19.430403421797177\n",
      "Iteration: 35, Distance: 20.060397138721758\n",
      "Iteration: 36, Distance: 14.870498814522316\n",
      "Iteration: 37, Distance: 19.928874722696325\n",
      "Iteration: 38, Distance: 16.008961453888045\n",
      "Iteration: 39, Distance: 18.021430151790753\n",
      "Iteration: 40, Distance: 16.34055236238287\n",
      "Iteration: 41, Distance: 17.85249157235751\n",
      "Iteration: 42, Distance: 18.04877024008326\n",
      "Iteration: 43, Distance: 16.327023397311002\n",
      "Iteration: 44, Distance: 23.392257398499364\n",
      "Iteration: 45, Distance: 20.981292778798423\n",
      "Iteration: 46, Distance: 19.223562029503494\n",
      "Iteration: 47, Distance: 26.763968032365355\n",
      "Iteration: 48, Distance: 20.755066806448212\n",
      "Iteration: 49, Distance: 19.613930137769135\n",
      "\n",
      " [[ 0.93583574 -0.16816837  0.30972707 -8.20442974]\n",
      " [ 0.22219133  0.96368844 -0.14810675 -9.36434494]\n",
      " [-0.27357352  0.20742226  0.93921964 19.6217741 ]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "T = ICP_alignment(pts1, pts2, num_iters=50)\n",
    "print(\"\\n\", T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.59234256e+01  7.95021551e+02 -1.57456766e+02  4.76566023e+03]\n",
      " [ 5.21788662e+01 -1.45423177e+03  1.61614182e+02 -1.90100843e+04]\n",
      " [-2.82227662e-01  2.67860258e-01  9.56548683e-01  2.08029605e+01]]\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"num_epochs\": 10,\n",
    "    \"tol\": 1e-3,\n",
    "    \"alpha\": 0.1\n",
    "}\n",
    "\n",
    "P_GN, costs = training_GN(pts, px, T[:3, :], args)\n",
    "print(P_GN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
